<div align="left">
<br>
<br>
</div>
<div align="center">
<h1>PCSNet: Prototypical Learning Guided Context-Aware Segmentation Network for Few-Shot Anomaly Detection</h1>
<div>
&nbsp;&nbsp;&nbsp;&nbsp;Yuxin Jiang<sup>1</sup>&emsp;
&nbsp;&nbsp;&nbsp;&nbsp;Yunkang Cao<sup>1, </sup>&emsp;
&nbsp;&nbsp;&nbsp;&nbsp;Weiming Shen<sup>1, *</sup>&emsp;
</div>
<div>
&nbsp;&nbsp;&nbsp;&nbsp;<sup>1</sup>Huazhong University of Science and Technology
</div>
  
[[Paper]](https://ieeexplore.ieee.org/document/10702559/)
  
![Framework](Figure/figure1.png)

---

</div>

>**Abstract:** Few-shot anomaly detection (FSAD) denotes the identification of anomalies within a target category with a limited number of normal samples. While pre-trained feature representations play an important role in existing FSAD methods, there exists a domain gap between pre-trained representations and target FSAD scenarios. This study proposes a Prototypical Learning Guided Context-Aware Segmentation Network (PCSNet) to address the domain gap and improve feature descriptiveness in target scenarios. In particular, PCSNet comprises a prototypical feature adaption (PFA) sub-network and a context-aware segmentation (CAS) sub-network. PFA extracts prototypical features as accurate guidance to ensure better feature compactness for normal data while distinct separation from anomalies. A pixel-level disparity classification loss is also designed to make subtle anomalies more distinguishable. Then a CAS sub-network is introduced for pixel-level anomaly localization, where pseudo anomalies are exploited to facilitate the training process. Experimental results on MVTec and MPDD demonstrate the superior FSAD performance of PCSNet, with 94.9% and 80.2% image-level AUROC in an 8-shot scenario, respectively. Real-world applications on automotive plastic part inspection further demonstrate that PCSNet can achieve promising results with limited training samples.

**Index Terms:** Anomaly detection; Pretrained feature representations; Few-shot learning; Prototypical learning;

## ğŸ’» Requirements
- pytorch == 1.12.0
- torchvision == 0.13.0
- numpy == 1.21.6
- scipy == 1.7.3
- matplotlib == 3.5.2
- tqdm

## ğŸ“¥ Dataset
Download the MVTec dataset [here](https://www.mvtec.com/company/research/datasets/mvtec-ad).

## ğŸš€ Usage
Execute the following command for training and evaluation:
```bash
python main.py
```

## ğŸ“Š Results (Image-level AUROC on MVTec AD)
| Category    | 2-shot          | 4-shot          | 8-shot          |
|-------------|-----------------|-----------------|-----------------|
| bottle      | 99.8Â±0.1       | 99.9Â±0.2       | 100.0Â±0.0      |
| cable       | 89.7Â±1.9       | 91.0Â±3.3       | 93.9Â±0.2       |
| capsule     | 72.3Â±8.9       | 76.5Â±10.5      | 88.6Â±1.5       |
| carpet      | 99.3Â±0.3       | 98.9Â±0.5       | 98.5Â±0.5       |
| grid        | 93.7Â±2.2       | 92.8Â±2.2       | 97.9Â±0.4       |
| hazelnut    | 95.1Â±2.0       | 98.6Â±2.0       | 99.9Â±0.1       |
| leather     | 100.0Â±0.0      | 100.0Â±0.0      | 100.0Â±0.0      |
| metal_nut   | 88.7Â±3.0       | 93.7Â±5.8       | 96.5Â±1.3       |
| pill        | 87.3Â±1.6       | 90.6Â±1.8       | 87.9Â±0.8       |
| screw       | 48.2Â±2.5       | 56.2Â±7.5       | 65.3Â±2.9       |
| tile        | 98.5Â±0.2       | 99.0Â±0.3       | 99.3Â±0.1       |
| toothbrush  | 87.8Â±0.7       | 90.6Â±5.4       | 98.3Â±0.2       |
| transistor  | 99.3Â±0.2       | 95.9Â±6.2       | 99.7Â±0.1       |
| wood        | 98.7Â±0.6       | 99.1Â±0.3       | 99.0Â±0.1       |
| zipper      | 98.3Â±0.7       | 99.0Â±0.4       | 98.8Â±0.2       |
| **average** | **90.4Â±0.6**   | **92.1Â±0.9**   | **94.9Â±0.2**   |

## ğŸ–¼ï¸ Visualization
![Results](https://github.com/yuxin-jiang/PCSNet/blob/main/Figure/Result.png)

## ğŸ“ Citation
If you find this work useful, please consider citing:
```
@article{jiang2024prototypical,
  title={Prototypical learning guided context-aware segmentation network for few-shot anomaly detection},
  author={Jiang, Yuxin and Cao, Yunkang and Shen, Weiming},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2024},
  publisher={IEEE}
}
```
